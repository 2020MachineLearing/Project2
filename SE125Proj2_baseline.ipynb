{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Baseline Implementation for SE125 Project 2\n",
    "\n",
    "We provide a baseline model for conversation modeling using deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries\n",
    "In this section, we import third-party libraries to be used in this project.\n",
    "You may need to install them using `pip`:\n",
    "```\n",
    "    pip install tqdm\n",
    "    pip install cython\n",
    "    pip install tables\n",
    "    pip install tensorboardX\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\cheng\\anaconda3\\lib\\site-packages (4.50.2)\n",
      "Requirement already satisfied: cython in c:\\users\\cheng\\anaconda3\\lib\\site-packages (0.29.21)\n",
      "Requirement already satisfied: tables in c:\\users\\cheng\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.9.3 in c:\\users\\cheng\\anaconda3\\lib\\site-packages (from tables) (1.19.1)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in c:\\users\\cheng\\anaconda3\\lib\\site-packages (from tables) (2.7.1)\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.1-py2.py3-none-any.whl (308 kB)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.14.0-py2.py3-none-any.whl (173 kB)\n",
      "Requirement already satisfied: six in c:\\users\\cheng\\anaconda3\\lib\\site-packages (from tensorboardX) (1.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\cheng\\anaconda3\\lib\\site-packages (from tensorboardX) (1.19.1)\n",
      "Installing collected packages: protobuf, tensorboardX\n",
      "Successfully installed protobuf-3.14.0 tensorboardX-2.1\n",
      "Requirement already satisfied: nltk in c:\\users\\cheng\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\cheng\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\cheng\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\users\\cheng\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cheng\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install cython\n",
    "!pip install tables\n",
    "!pip install tensorboardX\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-682d938db52a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import tables\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")#,format=\"%(asctime)s: %(name)s: %(levelname)s: %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilities\n",
    "\n",
    "In this section we maintain utilities for model construction and training. \n",
    "Please put your own utility modules/functions in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID, SOS_ID, EOS_ID, UNK_ID = [0, 1, 2, 3]\n",
    "\n",
    "def asHHMMSS(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    h = math.floor(m /60)\n",
    "    m -= h *60\n",
    "    return '%d:%d:%d'% (h, m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s<%s'%(asHHMMSS(s), asHHMMSS(rs))\n",
    "\n",
    "#######################################################################\n",
    "import nltk\n",
    "try: \n",
    "    nltk.word_tokenize(\"hello world\")\n",
    "except LookupError: \n",
    "    nltk.download('punkt')\n",
    "    \n",
    "def sent2indexes(sentence, vocab, maxlen):\n",
    "    '''sentence: a string or list of string\n",
    "       return: a numpy array of word indices\n",
    "    '''      \n",
    "    def convert_sent(sent, vocab, maxlen):\n",
    "        idxes = np.zeros(maxlen, dtype=np.int64)\n",
    "        idxes.fill(PAD_ID)\n",
    "        tokens = nltk.word_tokenize(sent.strip())\n",
    "        idx_len = min(len(tokens), maxlen)\n",
    "        for i in range(idx_len): idxes[i] = vocab.get(tokens[i], UNK_ID)\n",
    "        return idxes, idx_len\n",
    "    if type(sentence) is list:\n",
    "        inds, lens = [], []\n",
    "        for sent in sentence:\n",
    "            idxes, idx_len = convert_sent(sent, vocab, maxlen)\n",
    "            #idxes, idx_len = np.expand_dims(idxes, 0), np.array([idx_len])\n",
    "            inds.append(idxes)\n",
    "            lens.append(idx_len)\n",
    "        return np.vstack(inds), np.vstack(lens)\n",
    "    else:\n",
    "        inds, lens = sent2indexes([sentence], vocab, maxlen)\n",
    "        return inds[0], lens[0]\n",
    "\n",
    "def indexes2sent(indexes, vocab, ignore_tok=PAD_ID): \n",
    "    '''indexes: numpy array'''\n",
    "    def revert_sent(indexes, ivocab, ignore_tok=PAD_ID):\n",
    "        toks=[]\n",
    "        length=0\n",
    "        indexes=filter(lambda i: i!=ignore_tok, indexes)\n",
    "        for idx in indexes:\n",
    "            toks.append(ivocab[idx])\n",
    "            length+=1\n",
    "            if idx == EOS_ID:\n",
    "                break\n",
    "        return ' '.join(toks), length\n",
    "    \n",
    "    ivocab = {v: k for k, v in vocab.items()}\n",
    "    if indexes.ndim==1:# one sentence\n",
    "        return revert_sent(indexes, ivocab, ignore_tok)\n",
    "    else:# dim>1\n",
    "        sentences=[] # a batch of sentences\n",
    "        lens=[]\n",
    "        for inds in indexes:\n",
    "            sentence, length = revert_sent(inds, ivocab, ignore_tok)\n",
    "            sentences.append(sentence)\n",
    "            lens.append(length)\n",
    "        return sentences, lens\n",
    "    \n",
    "def save_model(model, epoch):\n",
    "    \"\"\"Save model parameters to checkpoint\"\"\"\n",
    "    ckpt_path=f'./output/checkpoint_iter{epoch}.pkl'\n",
    "    #print(f'Saving model parameters to {ckpt_path}')\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "        \n",
    "def load_model(model, epoch):\n",
    "    \"\"\"Load parameters from checkpoint\"\"\"\n",
    "    ckpt_path=f'./output/checkpoint_iter{epoch}.pkl'\n",
    "    #print(f'Loading model parameters from {ckpt_path}')\n",
    "    model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "In this section, we configurate some hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    conf = {\n",
    "    'maxlen':40, # maximum utterance length\n",
    "    'diaglen':10, # how many utterance kept in the context window\n",
    "\n",
    "    # Model Arguments\n",
    "    'emb_size':200, # size of word embeddings\n",
    "    'rnn_hid_utt':512, # number of rnn hidden units for utterance encoder\n",
    "    'rnn_hid_ctx':512, # number of rnn hidden units for context encoder\n",
    "    'rnn_hid_dec':512, # number of rnn hidden units for decoder\n",
    "    'n_layers':1, # number of layers\n",
    "    'dropout':0.5, # dropout applied to layers (0 = no dropout)\n",
    "    'teach_force': 0.8, # use teach force for decoder\n",
    "      \n",
    "    # Training Arguments\n",
    "    'batch_size':64,\n",
    "    'epochs':10, # maximum number of epochs\n",
    "    'lr':2e-4, # autoencoder learning rate\n",
    "    'beta1':0.9, # beta1 for adam\n",
    "    'init_w':0.05, # initial w\n",
    "    'clip':5.0,  # gradient clipping, max norm        \n",
    "    }\n",
    "    return conf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loader\n",
    "A tool to load batches from the binarized (.h5) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(data.Dataset):\n",
    "    def __init__(self, filepath, max_ctx_len=7, max_utt_len=40):\n",
    "        # 1. Initialize file path or list of file names.\n",
    "        \"\"\"read training sentences(list of int array) from a hdf5 file\"\"\"\n",
    "        self.max_ctx_len=max_ctx_len\n",
    "        self.max_utt_len=max_utt_len\n",
    "        \n",
    "        print(\"loading data...\")\n",
    "        table = tables.open_file(filepath)\n",
    "        self.data = table.get_node('/sentences')[:].astype(np.long)\n",
    "        self.index = table.get_node('/indices')[:]\n",
    "        self.data_len = self.index.shape[0]\n",
    "        print(\"{} entries\".format(self.data_len))\n",
    "\n",
    "    def __getitem__(self, offset):\n",
    "        pos_utt, ctx_len, res_len = self.index[offset]['pos_utt'], self.index[offset]['ctx_len'], self.index[offset]['res_len']\n",
    "        ctx_arr=self.data[pos_utt-ctx_len:pos_utt]\n",
    "        res_arr=self.data[pos_utt:pos_utt+res_len]\n",
    "        ## split context array into utterances\n",
    "        context=[]\n",
    "        utt_lens=[]\n",
    "        utt=[]\n",
    "        for i, tok in enumerate(ctx_arr):\n",
    "            utt.append(ctx_arr[i])\n",
    "            if tok==EOS_ID:\n",
    "                if len(utt)<self.max_utt_len+1:\n",
    "                    utt_lens.append(len(utt)-1)# floor is not counted in the utt length\n",
    "                    utt.extend([PAD_ID]*(self.max_utt_len+1-len(utt)))  \n",
    "                else:\n",
    "                    utt=utt[:self.max_utt_len+1]\n",
    "                    utt[-1]=EOS_ID\n",
    "                    utt_lens.append(self.max_utt_len)\n",
    "                context.append(utt)                \n",
    "                utt=[]    \n",
    "        if len(context)>self.max_ctx_len: # trunk long context\n",
    "            context=context[-self.max_ctx_len:]\n",
    "            utt_lens=utt_lens[-self.max_ctx_len:]\n",
    "        context_len=len(context)\n",
    "        \n",
    "        if len(context)<self.max_ctx_len: # pad short context\n",
    "            for i in range(len(context), self.max_ctx_len):\n",
    "                context.append([0, SOS_ID, EOS_ID]+[PAD_ID]*(self.max_utt_len-2)) # [floor, <sos>, <eos>, <pad>, <pad> ...]\n",
    "                utt_lens.append(2) # <s> and </s>\n",
    "        context = np.array(context)        \n",
    "        utt_lens=np.array(utt_lens)\n",
    "        floors=context[:,0]\n",
    "        context = context[:,1:]\n",
    "        \n",
    "        ## Padding ##    \n",
    "        response = res_arr[1:]\n",
    "        if len(response)<self.max_utt_len:\n",
    "            res_len=len(response)\n",
    "            response=np.append(response,[PAD_ID]*(self.max_utt_len-len(response)))\n",
    "        else:\n",
    "            response=response[:self.max_utt_len]\n",
    "            response[-1]=EOS_ID\n",
    "            res_len=self.max_utt_len\n",
    "\n",
    "        return context, context_len, utt_lens, floors, response, res_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "\n",
    "def load_dict(filename):\n",
    "    return json.loads(open(filename, \"r\").readline())\n",
    "\n",
    "def load_vecs(fin):         \n",
    "    \"\"\"read vectors (2D numpy array) from a hdf5 file\"\"\"\n",
    "    h5f = tables.open_file(fin)\n",
    "    h5vecs= h5f.root.vecs\n",
    "    \n",
    "    vecs=np.zeros(shape=h5vecs.shape,dtype=h5vecs.dtype)\n",
    "    vecs[:]=h5vecs[:]\n",
    "    h5f.close()\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Models\n",
    "Define your model(including its dependent sub-modules) here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as weight_init\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements embeddings of the words and adds their positional encodings. \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len = 50):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):   # for each position of the word\n",
    "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)   # include the batch size\n",
    "        return pe\n",
    "        \n",
    "    def forward(self, encoded_words):\n",
    "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
    "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, 512)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, 512)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
    "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
    "        weights = self.dropout(weights)\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        # (batch_size, max_len, h * d_k)\n",
    "        interacted = self.concat(context)\n",
    "        return interacted \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers, word_map):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embed = Embeddings(self.vocab_size, d_model)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_words, src_mask):\n",
    "        src_embeddings = self.embed(src_words)\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
    "        tgt_embeddings = self.embed(target_words)\n",
    "        for layer in self.decoder:\n",
    "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "        \n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
    "        return out\n",
    "\n",
    "# class RNNEncoder(nn.Module):\n",
    "#     def __init__(self, embedder, input_size, hidden_size, bidir, n_layers, dropout=0.5):\n",
    "#         super(RNNEncoder, self).__init__()\n",
    "        \n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.bidir = bidir\n",
    "#         assert type(self.bidir)==bool\n",
    "#         self.dropout=dropout\n",
    "        \n",
    "#         self.embedding = embedder # nn.Embedding(vocab_size, emb_size)\n",
    "#         self.rnn = nn.GRU(input_size, hidden_size, n_layers, batch_first=True, bidirectional=bidir)\n",
    "#         self.init_h = nn.Parameter(torch.randn(self.n_layers*(1+self.bidir), 1, self.hidden_size), requires_grad=True)#learnable h0\n",
    "#         self.init_weights()\n",
    "        \n",
    "#     def init_weights(self):\n",
    "#         \"\"\"adopted from https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5\"\"\"\n",
    "#         for w in self.rnn.parameters(): # initialize the gate weights with orthogonal\n",
    "#             if len(w.shape)>1: \n",
    "#                 weight_init.orthogonal_(w.data)\n",
    "    #         else:\n",
    "    #             weight_init.normal_(w.data)\n",
    "                \n",
    "    \n",
    "    # def forward(self, inputs, input_lens=None, init_h=None): \n",
    "    #     # init_h: [n_layers*n_dir x batch_size x hid_size]\n",
    "    #     if self.embedding is not None:\n",
    "    #         inputs=self.embedding(inputs)  # input: [batch_sz x seq_len] -> [batch_sz x seq_len x emb_sz]\n",
    "        \n",
    "    #     batch_size, seq_len, emb_size=inputs.size()\n",
    "    #     inputs=F.dropout(inputs, self.dropout, self.training)# dropout\n",
    "        \n",
    "    #     if input_lens is not None:# sort and pack sequence \n",
    "    #         input_lens_sorted, indices = input_lens.sort(descending=True)\n",
    "    #         inputs_sorted = inputs.index_select(0, indices)        \n",
    "    #         inputs = pack_padded_sequence(inputs_sorted, input_lens_sorted.data.tolist(), batch_first=True)\n",
    "        \n",
    "    #     if init_h is None:\n",
    "    #         init_h = self.init_h.expand(-1,batch_size,-1).contiguous()# use learnable initial states, expanding along batches\n",
    "    #     #self.rnn.flatten_parameters() # time consuming!!\n",
    "        # hids, h_n = self.rnn(inputs, init_h) # hids: [b x seq x (n_dir*hid_sz)]  \n",
    "        #                                           # h_n: [(n_layers*n_dir) x batch_sz x hid_sz] (2=fw&bw)\n",
    "        # if input_lens is not None: # reorder and pad\n",
    "        #     _, inv_indices = indices.sort()\n",
    "        #     hids, lens = pad_packed_sequence(hids, batch_first=True)     \n",
    "        #     hids = hids.index_select(0, inv_indices)\n",
    "        #     h_n = h_n.index_select(1, inv_indices)\n",
    "        # h_n = h_n.view(self.n_layers, (1+self.bidir), batch_size, self.hidden_size) #[n_layers x n_dirs x batch_sz x hid_sz]\n",
    "        # h_n = h_n[-1] # get the last layer [n_dirs x batch_sz x hid_sz]\n",
    "        # enc = h_n.view(batch_size,-1) #[batch_sz x (n_dirs*hid_sz)]\n",
    "            \n",
    "        # return enc, hids\n",
    "    \n",
    "# class ContextEncoder(nn.Module):\n",
    "#     def __init__(self, utt_encoder, input_size, hidden_size, n_layers=1, dropout=0.5):\n",
    "#         super(ContextEncoder, self).__init__()     \n",
    "#         self.utt_encoder=utt_encoder\n",
    "#         self.ctx_encoder= RNNEncoder(None, input_size, hidden_size, False, n_layers, dropout)\n",
    "\n",
    "#     def forward(self, context, context_lens, utt_lens): # context: [batch_sz x diag_len x max_utt_len] \n",
    "#                                                       # context_lens: [batch_sz x dia_len]\n",
    "#         batch_size, max_context_len, max_utt_len = context.size()\n",
    "#         utts = context.view(-1, max_utt_len) # [(batch_size*diag_len) x max_utt_len]\n",
    "#         utt_lens = utt_lens.view(-1)\n",
    "#         utt_encs, _ = self.utt_encoder(utts, utt_lens) # [(batch_size*diag_len) x 2hid_size]\n",
    "        \n",
    "#         utt_encs = utt_encs.view(batch_size, max_context_len, -1)\n",
    "#         enc, hids = self.ctx_encoder(utt_encs, context_lens)\n",
    "#         return enc\n",
    "  \n",
    "\n",
    "# class RNNDecoder(nn.Module):\n",
    "#     def __init__(self, embedder, input_size, hidden_size, vocab_size, n_layers=1, dropout=0.5):\n",
    "#         super(RNNDecoder, self).__init__()\n",
    "#         self.n_layers = n_layers\n",
    "#         self.input_size= input_size # size of the input to the RNN (e.g., embedding dim)\n",
    "    #     self.hidden_size = hidden_size # RNN hidden size\n",
    "    #     self.vocab_size = vocab_size # RNN output size (vocab size)\n",
    "    #     self.dropout= dropout\n",
    "        \n",
    "    #     self.embedding = embedder\n",
    "    #     self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "    #     self.project = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    #     self.init_weights()\n",
    "        \n",
    "    # def init_weights(self):\n",
    "    #     for w in self.rnn.parameters(): # initialize the gate weights with orthogonal\n",
    "    #         if w.dim()>1:\n",
    "    #             weight_init.orthogonal_(w)\n",
    "    #     self.project.weight.data.uniform_(-0.1, 0.1)#nn.init.xavier_normal_(self.out.weight)        \n",
    "    #     nn.init.constant_(self.project.bias, 0.)\n",
    "\n",
    "    # def forward(self, init_h, inputs=None, lens=None, enc_hids=None, src_pad_mask=None, context=None):\n",
    "    #     '''\n",
    "    #     init_h: initial hidden state for decoder\n",
    "    #     enc_hids: enc_hids for attention use\n",
    "    #     context: context information to be paired with input\n",
    "    #     inputs: inputs to the decoder\n",
    "    #     lens: input lengths\n",
    "    #     '''\n",
    "    #     if self.embedding is not None:\n",
    "    #         inputs = self.embedding(inputs) # input: [batch_sz x seqlen x emb_sz]\n",
    "    #     batch_size, maxlen, _ = inputs.size()\n",
    "    #     inputs = F.dropout(inputs, self.dropout, self.training)  \n",
    "    #     h = init_h.unsqueeze(0) # last_hidden of decoder [n_dir x batch_sz x hid_sz]        \n",
    "\n",
    "    #     if context is not None:            \n",
    "    #         repeated_context = context.unsqueeze(1).repeat(1, maxlen, 1) # [batch_sz x max_len x hid_sz]\n",
    "    #         inputs = torch.cat([inputs, repeated_context], 2)\n",
    "                \n",
    "    #         #self.rnn.flatten_parameters()\n",
    "    #     hids, h = self.rnn(inputs, h)         \n",
    "    #     decoded = self.project(hids.contiguous().view(-1, self.hidden_size))# reshape before linear over vocab\n",
    "    #     decoded = decoded.view(batch_size, maxlen, self.vocab_size)\n",
    "    #     return decoded, h\n",
    "    \n",
    "    # def sampling(self, init_h, enc_hids, src_pad_mask, context, maxlen, to_numpy=True):\n",
    "    #     \"\"\"\n",
    "    #     A simple greedy sampling\n",
    "    #     :param init_h: [batch_sz x hid_sz]\n",
    "    #     :param enc_hids: a tuple of (enc_hids, mask) for attention use. [batch_sz x seq_len x hid_sz]\n",
    "    #     \"\"\"\n",
    "    #     device = init_h.device\n",
    "    #     batch_size = init_h.size(0)\n",
    "    #     decoded_words = torch.zeros((batch_size, maxlen), dtype=torch.long, device=device)  \n",
    "    #     sample_lens = torch.zeros((batch_size), dtype=torch.long, device=device)\n",
    "    #     len_inc = torch.ones((batch_size), dtype=torch.long, device=device)\n",
    "               \n",
    "#         x = torch.zeros((batch_size, 1), dtype=torch.long, device=device).fill_(SOS_ID)# [batch_sz x 1] (1=seq_len)\n",
    "#         h = init_h.unsqueeze(0) # [1 x batch_sz x hid_sz]  \n",
    "#         for di in range(maxlen):  \n",
    "#             if self.embedding is not None:\n",
    "#                 x = self.embedding(x) # x: [batch_sz x 1 x emb_sz]\n",
    "#             h_n, h = self.rnn(x, h) # h_n: [batch_sz x 1 x hid_sz] h: [1 x batch_sz x hid_sz]\n",
    "\n",
    "#             logits = self.project(h_n) # out: [batch_sz x 1 x vocab_sz]  \n",
    "#             logits = logits.squeeze(1) # [batch_size x vocab_size]                  \n",
    "#             x = torch.multinomial(F.softmax(logits, dim=1), 1)  # [batch_size x 1 x 1]?\n",
    "#             decoded_words[:,di] = x.squeeze()\n",
    "#             len_inc=len_inc*(x.squeeze()!=EOS_ID).long() # stop increse length (set 0 bit) when EOS is met\n",
    "#             sample_lens=sample_lens+len_inc            \n",
    "        \n",
    "#         if to_numpy:\n",
    "#             decoded_words = decoded_words.data.cpu().numpy()\n",
    "#             sample_lens = sample_lens.data.cpu().numpy()\n",
    "#         return decoded_words, sample_lens\n",
    "\n",
    "# class MyModel(nn.Module):\n",
    "#     '''The basic Hierarchical Recurrent Encoder-Decoder model. '''\n",
    "#     def __init__(self, config, vocab_size):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.vocab_size = vocab_size \n",
    "#         self.maxlen=config['maxlen']\n",
    "    #     self.clip = config['clip']\n",
    "    #     self.init_w = config['init_w']\n",
    "        \n",
    "    #     self.embedder= nn.Embedding(vocab_size, config['emb_size'], padding_idx=PAD_ID)\n",
    "    #     self.utt_encoder = RNNEncoder(self.embedder, config['emb_size'], config['rnn_hid_utt'], True, \n",
    "    #                                config['n_layers'], config['dropout']) \n",
    "    #                                                     # utter encoder: encode response to vector\n",
    "    #     self.context_encoder = ContextEncoder(self.utt_encoder, config['rnn_hid_utt']*2,\n",
    "    #                                           config['rnn_hid_ctx'], 1, config['dropout']) \n",
    "    #                                           # context encoder: encode context to vector    \n",
    "    #     self.decoder = RNNDecoder(self.embedder, config['emb_size'], config['rnn_hid_ctx'], vocab_size, 1, config['dropout']) # utter decoder: P(x|c,z)\n",
    "    #     self.optimizer = optim.Adam(list(self.context_encoder.parameters())\n",
    "    #                                   +list(self.decoder.parameters()),lr=config['lr'])\n",
    "\n",
    "    # def forward(self, context, context_lens, utt_lens, response, res_lens):\n",
    "    #     c = self.context_encoder(context, context_lens, utt_lens)\n",
    "    #     output,_ = self.decoder(c, response[:,:-1], res_lens-1) # decode from z, c  # output: [batch x seq_len x n_tokens]   \n",
    "    #     dec_target = response[:,1:].clone()\n",
    "    #     dec_target[response[:,1:]==PAD_ID] = -100\n",
    "    #     loss = nn.CrossEntropyLoss()(output.view(-1, self.vocab_size), dec_target.view(-1))\n",
    "    #     return loss\n",
    "    \n",
    "    # def train_batch(self, context, context_lens, utt_lens, response, res_lens):\n",
    "    #     self.context_encoder.train()\n",
    "    #     self.decoder.train()\n",
    "        \n",
    "    #     loss = self.forward(context, context_lens, utt_lens, response, res_lens)\n",
    "        \n",
    "    #     self.optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     # `clip_grad_norm` to prevent exploding gradient in RNNs\n",
    "    #     nn.utils.clip_grad_norm_(list(self.context_encoder.parameters())+list(self.decoder.parameters()), self.clip)\n",
    "    #     self.optimizer.step()\n",
    "        \n",
    "    #     return {'train_loss': loss.item()}      \n",
    "    \n",
    "    # def valid(self, context, context_lens, utt_lens, response, res_lens):\n",
    "    #     self.context_encoder.eval()  \n",
    "    #     self.decoder.eval()        \n",
    "    #     loss = self.forward(context, context_lens, utt_lens, response, res_lens)\n",
    "    #     return {'valid_loss': loss.item()}\n",
    "    \n",
    "    # def sample(self, context, context_lens, utt_lens, n_samples):    \n",
    "    #     self.context_encoder.eval()\n",
    "    #     self.decoder.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         c = self.context_encoder(context, context_lens, utt_lens)\n",
    "    #     sample_words, sample_lens = self.decoder.sampling(c, None, None, None, n_samples, self.maxlen)  \n",
    "    #     return sample_words, sample_lens  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "We provide the evaluation script as well as the BLEU score metric. \n",
    "\n",
    "**Do not change code in this block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from collections import Counter\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Metrics, self).__init__()\n",
    "\n",
    "    def sim_bleu(self, hyps, ref):\n",
    "        \"\"\"\n",
    "        :param ref - a list of tokens of the reference\n",
    "        :param hyps - a list of tokens of the hypothesis\n",
    "    \n",
    "        :return maxbleu - recall bleu\n",
    "        :return avgbleu - precision bleu\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for hyp in hyps:\n",
    "            try:\n",
    "                scores.append(sentence_bleu([ref], hyp, smoothing_function=SmoothingFunction().method7,\n",
    "                                        weights=[1./4, 1./4, 1./4, 1./4]))\n",
    "            except:\n",
    "                scores.append(0.0)\n",
    "        return np.max(scores), np.mean(scores)\n",
    "    \n",
    "def evaluate(model, metrics, test_loader, vocab, repeat, f_eval):\n",
    "    ivocab = {v: k for k, v in vocab.items()}\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    recall_bleus, prec_bleus, avg_lens  = [], [], []\n",
    "        \n",
    "    dlg_id = 0\n",
    "    for context, context_lens, utt_lens, floors, response, res_lens in tqdm(test_loader): \n",
    "        \n",
    "        if dlg_id > 5000: break\n",
    "        \n",
    "#        max_ctx_len = max(context_lens)\n",
    "        max_ctx_len = context.size(1)\n",
    "        context, utt_lens, floors = context[:,:max_ctx_len,1:], utt_lens[:,:max_ctx_len]-1, floors[:,:max_ctx_len] \n",
    "                         # remove empty utts and the sos token in the context and reduce the context length\n",
    "        ctx, ctx_lens = context, context_lens\n",
    "        context, context_lens, utt_lens \\\n",
    "            = [tensor.to(device) for tensor in [context, context_lens, utt_lens]]\n",
    "\n",
    "#################################################\n",
    "        utt_lens[utt_lens<=0]=1\n",
    "#################################################\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sample_words, sample_lens = model.sample(context, context_lens, utt_lens, repeat)\n",
    "        # nparray: [repeat x seq_len]       \n",
    "        \n",
    "        pred_sents, _ = indexes2sent(sample_words, vocab)\n",
    "        pred_tokens = [sent.split(' ') for sent in pred_sents]   \n",
    "        ref_str, _ =indexes2sent(response[0].numpy(), vocab, SOS_ID)\n",
    "        #ref_str = ref_str.encode('utf-8')\n",
    "        ref_tokens = ref_str.split(' ')\n",
    "        \n",
    "        max_bleu, avg_bleu = metrics.sim_bleu(pred_tokens, ref_tokens)\n",
    "        recall_bleus.append(max_bleu)\n",
    "        prec_bleus.append(avg_bleu)\n",
    "        \n",
    "        avg_lens.append(np.mean(sample_lens))\n",
    "\n",
    "        response, res_lens = [tensor.to(device) for tensor in [response, res_lens]]\n",
    "        \n",
    "        ## Write concrete results to a text file\n",
    "        dlg_id += 1 \n",
    "        if f_eval is not None:\n",
    "            f_eval.write(\"Batch {:d} \\n\".format(dlg_id))\n",
    "            # print the context\n",
    "            start = np.maximum(0, ctx_lens[0]-5)\n",
    "            for t_id in range(start, ctx_lens[0], 1):\n",
    "                context_str = indexes2sent(ctx[0, t_id].numpy(), vocab)\n",
    "                f_eval.write(\"Context {:d}-{:d}: {}\\n\".format(t_id, floors[0, t_id], context_str))\n",
    "            #print the ground truth response    \n",
    "            f_eval.write(\"Target >> {}\\n\".format(ref_str.replace(\" ' \", \"'\")))\n",
    "            for res_id, pred_sent in enumerate(pred_sents):\n",
    "                f_eval.write(\"Sample {:d} >> {}\\n\".format(res_id, pred_sent.replace(\" ' \", \"'\")))\n",
    "            f_eval.write(\"\\n\")\n",
    "    prec_bleu= float(np.mean(prec_bleus))\n",
    "    recall_bleu = float(np.mean(recall_bleus))\n",
    "    result = {'avg_len':float(np.mean(avg_lens)),\n",
    "              'recall_bleu': recall_bleu, 'prec_bleu': prec_bleu, \n",
    "              'f1_bleu': 2*(prec_bleu*recall_bleu) / (prec_bleu+recall_bleu+10e-12),\n",
    "             }\n",
    "    \n",
    "    if f_eval is not None:\n",
    "        for k, v in result.items():\n",
    "            f_eval.write(str(k) + ':'+ str(v)+' ')\n",
    "        f_eval.write('\\n')\n",
    "    print(\"Done testing\")\n",
    "    print(result)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "The training script here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter # install tensorboardX (pip install tensorboardX) before importing this package\n",
    "\n",
    "def train(args, model=None, pad = 0):\n",
    "    # LOG #\n",
    "    fh = logging.FileHandler(f\"./output/logs.txt\")\n",
    "                                      # create file handler which logs even debug messages\n",
    "    logger.addHandler(fh)# add the handlers to the logger\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M')\n",
    "    tb_writer = SummaryWriter(f\"./output/logs/{timestamp}\") if args.visual else None\n",
    "\n",
    "    # Set the random seed manually for reproducibility.\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    device = torch.device(f\"cuda:{args.gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "\n",
    "    config=get_config()\n",
    "\n",
    "    if args.visual:\n",
    "        json.dump(config, open(f'./output/config_{timestamp}.json', 'w'))# save configs\n",
    "\n",
    "    ###############################################################################\n",
    "    # Load data\n",
    "    ###############################################################################\n",
    "    data_path = args.data_path+args.dataset+'/'\n",
    "    train_set = DialogDataset(os.path.join(data_path, 'train.h5'), config['diaglen'], config['maxlen'])\n",
    "    valid_set = DialogDataset(os.path.join(data_path, 'valid.h5'), config['diaglen'], config['maxlen'])\n",
    "    test_set = DialogDataset(os.path.join(data_path, 'test.h5'), config['diaglen'], config['maxlen'])\n",
    "    vocab = load_dict(os.path.join(data_path, 'vocab.json'))\n",
    "    ivocab = {v: k for k, v in vocab.items()}\n",
    "    n_tokens = len(ivocab)\n",
    "    metrics=Metrics()    \n",
    "    print(\"Loaded data!\")\n",
    "\n",
    "    ###############################################################################\n",
    "    # Define the models\n",
    "    ###############################################################################\n",
    "    if model is None:\n",
    "        model = MyModel(config, n_tokens)\n",
    "\n",
    "    if args.reload_from>=0:\n",
    "        load_model(model, args.reload_from)\n",
    "        \n",
    "    model=model.to(device)\n",
    "\n",
    "    logger.info(\"Training...\")\n",
    "    best_perf = -1\n",
    "    itr_global=1\n",
    "    start_epoch=1 if args.reload_from==-1 else args.reload_from+1\n",
    "    for epoch in range(start_epoch, config['epochs']+1):\n",
    "        epoch_start_time = time.time()\n",
    "        itr_start_time = time.time()\n",
    "        \n",
    "        # shuffle (re-define) data between epochs   \n",
    "        train_loader=torch.utils.data.DataLoader(dataset=train_set, batch_size=config['batch_size'],\n",
    "                                                 shuffle=True, num_workers=1, drop_last=True)\n",
    "        n_iters=train_loader.__len__()\n",
    "        itr = 1\n",
    "        for batch in train_loader:# loop through all batches in training data\n",
    "            model.train()\n",
    "            context, context_lens, utt_lens, floors, response, res_lens = batch\n",
    "\n",
    " #           max_ctx_len = max(context_lens)\n",
    "            max_ctx_len = context.size(1)\n",
    "            context, utt_lens = context[:,:max_ctx_len,1:], utt_lens[:,:max_ctx_len]-1\n",
    "                                    # remove empty utterances in context\n",
    "                                    # remove the sos token in the context and reduce the context length     \n",
    "#################################################\n",
    "            utt_lens[utt_lens<=0]=1\n",
    "#################################################\n",
    "            batch_gpu = [tensor.to(device) for tensor in [context, context_lens, utt_lens, response, res_lens]] \n",
    "            train_results = model.train_batch(*batch_gpu)\n",
    "                     \n",
    "            if itr % args.log_every == 0:\n",
    "                elapsed = time.time() - itr_start_time\n",
    "                log = '%s|%s@gpu%d epo:[%d/%d] iter:[%d/%d] step_time:%ds elapsed:%s'\\\n",
    "                %(args.model, args.dataset, args.gpu_id, epoch, config['epochs'],\n",
    "                         itr, n_iters, elapsed, timeSince(epoch_start_time,itr/n_iters))\n",
    "                logger.info(log)\n",
    "                logger.info(train_results)\n",
    "                if args.visual:\n",
    "                    tb_writer.add_scalar('train_loss', train_results['train_loss'], itr_global)\n",
    "\n",
    "                itr_start_time = time.time()    \n",
    "                \n",
    "            if itr % args.valid_every == 0 and False:\n",
    "                logger.info('Validation ')\n",
    "                valid_loader=torch.utils.data.DataLoader(dataset=valid_set, batch_size=config['batch_size'], shuffle=True, num_workers=1)\n",
    "                model.eval()    \n",
    "                valid_losses = []\n",
    "                for context, context_lens, utt_lens, floors, response, res_lens in valid_loader:\n",
    " #                   max_ctx_len = max(context_lens)\n",
    "                    max_ctx_len = context.size(1)\n",
    "                    context, utt_lens = context[:,:max_ctx_len,1:], utt_lens[:,:max_ctx_len]-1\n",
    "                             # remove empty utterances in context\n",
    "                             # remove the sos token in the context and reduce the context length\n",
    "#################################################\n",
    "                    utt_lens[utt_lens<=0]=1\n",
    "#################################################\n",
    "                    batch = [tensor.to(device) for tensor in [context, context_lens, utt_lens, response, res_lens]]\n",
    "                    valid_results = model.valid(*batch)    \n",
    "                    valid_losses.append(valid_results['valid_loss'])\n",
    "                if args.visual: tb_writer.add_scalar('valid_loss', np.mean(valid_losses), itr_global)\n",
    "                logger.info({'valid_loss':np.mean(valid_losses)})    \n",
    "                \n",
    "            itr += 1\n",
    "            itr_global+=1            \n",
    "            \n",
    "            if itr_global % args.eval_every == 0:  # evaluate the model in the validation set\n",
    "                model.eval()          \n",
    "                logger.info(\"Evaluating in the validation set..\")\n",
    "\n",
    "                valid_loader=torch.utils.data.DataLoader(dataset=valid_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "                f_eval = open(f\"./output/tmp_results/iter{itr_global}.txt\", \"w\")\n",
    "                repeat = 10            \n",
    "                eval_results = evaluate(model, metrics, valid_loader, vocab, repeat, f_eval)\n",
    "                bleu = eval_results['recall_bleu']\n",
    "                if bleu> best_perf:\n",
    "                    save_model(model, 0)#itr_global) # save model after each epoch\n",
    "                if args.visual:\n",
    "                    tb_writer.add_scalar('recall_bleu', bleu, itr_global)\n",
    "                \n",
    "        # end of epoch ----------------------------\n",
    "               # model.adjust_lr()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Function (Training)\n",
    "You can change the default arguments by setting the `default` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': './data/', 'model': 'MyModel', 'dataset': 'dailydialog', 'visual': True, 'reload_from': -1, 'gpu_id': 0, 'log_every': 100, 'valid_every': 1000, 'eval_every': 2000, 'seed': 1111}\n",
      "cuda:0\n",
      "loading data...\n",
      "76052 entries\n",
      "loading data...\n",
      "7069 entries\n",
      "loading data...\n",
      "6740 entries\n",
      "Loaded data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[100/1188] step_time:4s elapsed:0:0:4<0:0:51\n",
      "{'train_loss': 5.200043678283691}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[200/1188] step_time:4s elapsed:0:0:9<0:0:45\n",
      "{'train_loss': 5.231085777282715}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[300/1188] step_time:4s elapsed:0:0:13<0:0:39\n",
      "{'train_loss': 4.976045608520508}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[400/1188] step_time:4s elapsed:0:0:17<0:0:35\n",
      "{'train_loss': 4.917187690734863}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[500/1188] step_time:4s elapsed:0:0:22<0:0:30\n",
      "{'train_loss': 4.728878498077393}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[600/1188] step_time:4s elapsed:0:0:26<0:0:26\n",
      "{'train_loss': 4.5007171630859375}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[700/1188] step_time:4s elapsed:0:0:31<0:0:21\n",
      "{'train_loss': 4.743600845336914}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[800/1188] step_time:4s elapsed:0:0:35<0:0:17\n",
      "{'train_loss': 4.668554306030273}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[900/1188] step_time:4s elapsed:0:0:39<0:0:12\n",
      "{'train_loss': 4.317011833190918}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[1000/1188] step_time:4s elapsed:0:0:44<0:0:8\n",
      "{'train_loss': 4.644478797912598}\n",
      "MyModel|dailydialog@gpu0 epo:[1/10] iter:[1100/1188] step_time:4s elapsed:0:0:48<0:0:3\n",
      "{'train_loss': 4.386983394622803}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[100/1188] step_time:4s elapsed:0:0:4<0:0:49\n",
      "{'train_loss': 4.4155473709106445}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[200/1188] step_time:4s elapsed:0:0:8<0:0:44\n",
      "{'train_loss': 4.372920513153076}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[300/1188] step_time:4s elapsed:0:0:13<0:0:39\n",
      "{'train_loss': 4.247354507446289}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[400/1188] step_time:4s elapsed:0:0:17<0:0:34\n",
      "{'train_loss': 4.191388130187988}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[500/1188] step_time:4s elapsed:0:0:22<0:0:30\n",
      "{'train_loss': 4.317567825317383}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[600/1188] step_time:4s elapsed:0:0:26<0:0:25\n",
      "{'train_loss': 4.496272087097168}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[700/1188] step_time:4s elapsed:0:0:30<0:0:21\n",
      "{'train_loss': 4.2750043869018555}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[800/1188] step_time:4s elapsed:0:0:35<0:0:17\n",
      "{'train_loss': 4.360223770141602}\n",
      "Evaluating in the validation set..\n",
      " 71%|   | 5001/7069 [01:22<00:34, 60.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done testing\n",
      "{'avg_len': 8.406918616276744, 'recall_bleu': 0.2929561883352304, 'prec_bleu': 0.2929561883352304, 'f1_bleu': 0.2929561883302304}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[900/1188] step_time:87s elapsed:0:2:2<0:0:39\n",
      "{'train_loss': 4.337692737579346}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[1000/1188] step_time:4s elapsed:0:2:7<0:0:23\n",
      "{'train_loss': 4.3698811531066895}\n",
      "MyModel|dailydialog@gpu0 epo:[2/10] iter:[1100/1188] step_time:4s elapsed:0:2:11<0:0:10\n",
      "{'train_loss': 4.231498718261719}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[100/1188] step_time:4s elapsed:0:0:4<0:0:48\n",
      "{'train_loss': 4.170657634735107}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[200/1188] step_time:4s elapsed:0:0:8<0:0:43\n",
      "{'train_loss': 3.9518911838531494}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[300/1188] step_time:4s elapsed:0:0:13<0:0:39\n",
      "{'train_loss': 4.0901665687561035}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[400/1188] step_time:4s elapsed:0:0:17<0:0:35\n",
      "{'train_loss': 4.005061149597168}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[500/1188] step_time:4s elapsed:0:0:22<0:0:30\n",
      "{'train_loss': 4.132946014404297}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[600/1188] step_time:4s elapsed:0:0:26<0:0:26\n",
      "{'train_loss': 3.955376148223877}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[700/1188] step_time:4s elapsed:0:0:31<0:0:21\n",
      "{'train_loss': 3.8828399181365967}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[800/1188] step_time:4s elapsed:0:0:35<0:0:17\n",
      "{'train_loss': 4.00427770614624}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[900/1188] step_time:4s elapsed:0:0:40<0:0:12\n",
      "{'train_loss': 4.16484260559082}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[1000/1188] step_time:4s elapsed:0:0:44<0:0:8\n",
      "{'train_loss': 3.9931602478027344}\n",
      "MyModel|dailydialog@gpu0 epo:[3/10] iter:[1100/1188] step_time:4s elapsed:0:0:49<0:0:3\n",
      "{'train_loss': 3.8979949951171875}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[100/1188] step_time:4s elapsed:0:0:4<0:0:48\n",
      "{'train_loss': 4.077713966369629}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[200/1188] step_time:4s elapsed:0:0:8<0:0:43\n",
      "{'train_loss': 3.9104084968566895}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[300/1188] step_time:4s elapsed:0:0:13<0:0:39\n",
      "{'train_loss': 3.8564162254333496}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[400/1188] step_time:4s elapsed:0:0:17<0:0:34\n",
      "{'train_loss': 3.8563947677612305}\n",
      "Evaluating in the validation set..\n",
      " 71%|   | 5001/7069 [01:22<00:34, 60.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done testing\n",
      "{'avg_len': 8.549090181963606, 'recall_bleu': 0.28750058088348457, 'prec_bleu': 0.28750058088348457, 'f1_bleu': 0.28750058087848457}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[500/1188] step_time:87s elapsed:0:1:44<0:2:24\n",
      "{'train_loss': 3.8393564224243164}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[600/1188] step_time:4s elapsed:0:1:49<0:1:47\n",
      "{'train_loss': 3.918208122253418}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[700/1188] step_time:4s elapsed:0:1:53<0:1:19\n",
      "{'train_loss': 3.901686668395996}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[800/1188] step_time:4s elapsed:0:1:58<0:0:57\n",
      "{'train_loss': 3.920520067214966}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[900/1188] step_time:4s elapsed:0:2:2<0:0:39\n",
      "{'train_loss': 3.9128811359405518}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[1000/1188] step_time:4s elapsed:0:2:6<0:0:23\n",
      "{'train_loss': 3.697035312652588}\n",
      "MyModel|dailydialog@gpu0 epo:[4/10] iter:[1100/1188] step_time:4s elapsed:0:2:11<0:0:10\n",
      "{'train_loss': 3.7917532920837402}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[100/1188] step_time:4s elapsed:0:0:4<0:0:48\n",
      "{'train_loss': 3.53613018989563}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[200/1188] step_time:4s elapsed:0:0:8<0:0:43\n",
      "{'train_loss': 3.7533628940582275}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[300/1188] step_time:4s elapsed:0:0:13<0:0:39\n",
      "{'train_loss': 3.8125765323638916}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[400/1188] step_time:4s elapsed:0:0:17<0:0:34\n",
      "{'train_loss': 3.8881731033325195}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[500/1188] step_time:4s elapsed:0:0:22<0:0:30\n",
      "{'train_loss': 3.8107597827911377}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[600/1188] step_time:4s elapsed:0:0:26<0:0:25\n",
      "{'train_loss': 3.9270694255828857}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[700/1188] step_time:4s elapsed:0:0:30<0:0:21\n",
      "{'train_loss': 3.8400719165802}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[800/1188] step_time:4s elapsed:0:0:35<0:0:17\n",
      "{'train_loss': 3.933643341064453}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[900/1188] step_time:4s elapsed:0:0:39<0:0:12\n",
      "{'train_loss': 3.7724804878234863}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[1000/1188] step_time:4s elapsed:0:0:44<0:0:8\n",
      "{'train_loss': 3.7636075019836426}\n",
      "MyModel|dailydialog@gpu0 epo:[5/10] iter:[1100/1188] step_time:4s elapsed:0:0:48<0:0:3\n",
      "{'train_loss': 3.960055351257324}\n",
      "Evaluating in the validation set..\n",
      " 71%|   | 5001/7069 [01:23<00:34, 60.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done testing\n",
      "{'avg_len': 8.582683463307339, 'recall_bleu': 0.2826565100775758, 'prec_bleu': 0.2826565100775758, 'f1_bleu': 0.2826565100725758}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[100/1188] step_time:87s elapsed:0:1:27<0:15:54\n",
      "{'train_loss': 3.657320976257324}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[200/1188] step_time:4s elapsed:0:1:32<0:7:35\n",
      "{'train_loss': 3.791257619857788}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[300/1188] step_time:4s elapsed:0:1:36<0:4:45\n",
      "{'train_loss': 3.865710973739624}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[400/1188] step_time:4s elapsed:0:1:40<0:3:18\n",
      "{'train_loss': 3.8020360469818115}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[500/1188] step_time:4s elapsed:0:1:45<0:2:25\n",
      "{'train_loss': 3.8356077671051025}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[600/1188] step_time:4s elapsed:0:1:49<0:1:47\n",
      "{'train_loss': 3.495431900024414}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[700/1188] step_time:4s elapsed:0:1:54<0:1:19\n",
      "{'train_loss': 3.692436456680298}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[800/1188] step_time:4s elapsed:0:1:58<0:0:57\n",
      "{'train_loss': 3.315817356109619}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[900/1188] step_time:4s elapsed:0:2:3<0:0:39\n",
      "{'train_loss': 3.6919498443603516}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[1000/1188] step_time:4s elapsed:0:2:7<0:0:23\n",
      "{'train_loss': 3.8041229248046875}\n",
      "MyModel|dailydialog@gpu0 epo:[6/10] iter:[1100/1188] step_time:4s elapsed:0:2:12<0:0:10\n",
      "{'train_loss': 3.838655710220337}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[100/1188] step_time:4s elapsed:0:0:4<0:0:48\n",
      "{'train_loss': 3.5565712451934814}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[200/1188] step_time:4s elapsed:0:0:8<0:0:44\n",
      "{'train_loss': 3.571685791015625}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[300/1188] step_time:4s elapsed:0:0:13<0:0:39\n",
      "{'train_loss': 3.73825740814209}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[400/1188] step_time:4s elapsed:0:0:17<0:0:35\n",
      "{'train_loss': 3.5251643657684326}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[500/1188] step_time:4s elapsed:0:0:22<0:0:30\n",
      "{'train_loss': 3.701815605163574}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[600/1188] step_time:4s elapsed:0:0:26<0:0:26\n",
      "{'train_loss': 3.641826868057251}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[700/1188] step_time:4s elapsed:0:0:31<0:0:21\n",
      "{'train_loss': 3.544004201889038}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[800/1188] step_time:4s elapsed:0:0:35<0:0:17\n",
      "{'train_loss': 3.6053059101104736}\n",
      "Evaluating in the validation set..\n",
      " 71%|   | 5001/7069 [01:25<00:35, 58.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done testing\n",
      "{'avg_len': 8.660867826434712, 'recall_bleu': 0.28624844655458925, 'prec_bleu': 0.28624844655458925, 'f1_bleu': 0.28624844654958925}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[900/1188] step_time:89s elapsed:0:2:5<0:0:40\n",
      "{'train_loss': 3.734748601913452}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[1000/1188] step_time:4s elapsed:0:2:10<0:0:24\n",
      "{'train_loss': 3.532738447189331}\n",
      "MyModel|dailydialog@gpu0 epo:[7/10] iter:[1100/1188] step_time:4s elapsed:0:2:14<0:0:10\n",
      "{'train_loss': 3.6076507568359375}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[100/1188] step_time:4s elapsed:0:0:4<0:0:50\n",
      "{'train_loss': 3.6799912452697754}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[200/1188] step_time:4s elapsed:0:0:9<0:0:44\n",
      "{'train_loss': 3.6258010864257812}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[300/1188] step_time:4s elapsed:0:0:13<0:0:39\n",
      "{'train_loss': 3.4180774688720703}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[400/1188] step_time:4s elapsed:0:0:17<0:0:35\n",
      "{'train_loss': 3.686941146850586}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[500/1188] step_time:4s elapsed:0:0:22<0:0:30\n",
      "{'train_loss': 3.5986545085906982}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[600/1188] step_time:4s elapsed:0:0:26<0:0:26\n",
      "{'train_loss': 3.656144142150879}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[700/1188] step_time:4s elapsed:0:0:31<0:0:21\n",
      "{'train_loss': 3.4819140434265137}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[800/1188] step_time:4s elapsed:0:0:35<0:0:17\n",
      "{'train_loss': 3.689035177230835}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[900/1188] step_time:4s elapsed:0:0:40<0:0:12\n",
      "{'train_loss': 3.4831624031066895}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[1000/1188] step_time:4s elapsed:0:0:44<0:0:8\n",
      "{'train_loss': 3.6648428440093994}\n",
      "MyModel|dailydialog@gpu0 epo:[8/10] iter:[1100/1188] step_time:4s elapsed:0:0:48<0:0:3\n",
      "{'train_loss': 3.7448654174804688}\n",
      "MyModel|dailydialog@gpu0 epo:[9/10] iter:[100/1188] step_time:4s elapsed:0:0:4<0:0:49\n",
      "{'train_loss': 3.5779271125793457}\n",
      "MyModel|dailydialog@gpu0 epo:[9/10] iter:[200/1188] step_time:4s elapsed:0:0:9<0:0:44\n",
      "{'train_loss': 3.5189690589904785}\n",
      "MyModel|dailydialog@gpu0 epo:[9/10] iter:[300/1188] step_time:4s elapsed:0:0:13<0:0:39\n",
      "{'train_loss': 3.802725076675415}\n",
      "MyModel|dailydialog@gpu0 epo:[9/10] iter:[400/1188] step_time:4s elapsed:0:0:17<0:0:35\n",
      "{'train_loss': 3.504763603210449}\n",
      "Evaluating in the validation set..\n",
      " 25%|       | 1740/7069 [00:29<01:30, 58.91it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Dialog Pytorch')\n",
    "    # Path Arguments\n",
    "    parser.add_argument('--data_path', type=str, default='./data/', help='location of the data corpus')\n",
    "    parser.add_argument('--model', type=str, default='MyModel', help='model name')\n",
    "    # parser.add_argument('--dataset', type=str, default='weibo', help='name of dataset.')\n",
    "    parser.add_argument('--dataset', type=str, default='dailydialog', help='name of dataset.')\n",
    "    # parser.add_argument('-v','--visual', action='store_true', default=False, help='visualize training status in tensorboard')\n",
    "    parser.add_argument('-v','--visual', action='store_true', default=True, help='visualize training status in tensorboard')\n",
    "    parser.add_argument('--reload_from', type=int, default=-1, help='reload from a trained ephoch')\n",
    "    # parser.add_argument('--gpu_id', type=int, default=1, help='GPU ID')\n",
    "    parser.add_argument('--gpu_id', type=int, default=0, help='GPU ID')\n",
    "\n",
    "    # Evaluation Arguments\n",
    "    parser.add_argument('--log_every', type=int, default=100, help='interval to log autoencoder training results')\n",
    "    parser.add_argument('--valid_every', type=int, default=1000, help='interval to validation')\n",
    "    parser.add_argument('--eval_every', type=int, default=2000, help='interval to evaluation to concrete results')\n",
    "    parser.add_argument('--seed', type=int, default=1111, help='random seed')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    print(vars(args))\n",
    "\n",
    "    # make output directory if it doesn't already exist\n",
    "    os.makedirs(f'./output/models', exist_ok=True)\n",
    "    os.makedirs(f'./output/tmp_results', exist_ok=True)\n",
    "        \n",
    "    torch.backends.cudnn.benchmark = True # speed up training by using cudnn\n",
    "    torch.backends.cudnn.deterministic = True # fix the random seed in cudnn\n",
    "    \n",
    "    model = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Function (Test)\n",
    "\n",
    "**Please do not change code here except the default arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': './data/', 'dataset': 'weibo', 'model': 'MyModel', 'reload_from': 0, 'n_samples': 10, 'seed': 1111}\n",
      "loading data...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "``./data/weibo/test.h5`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ba3e5c77d9d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-ba3e5c77d9d8>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDialogDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diaglen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxlen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'vocab.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c122ac99444e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath, max_ctx_len, max_utt_len)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/sentences'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/indices'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Finally, create the File instance, and return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/tables/file.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/tables/utils.py\u001b[0m in \u001b[0;36mcheck_file_access\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# The file should be readable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF_OK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` does not exist\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` is not a regular file\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ``./data/weibo/test.h5`` does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "def test(args):\n",
    "    conf = get_config()\n",
    "    # Set the random seed manually for reproducibility.\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    else:\n",
    "        print(\"Note that our pre-trained models require CUDA to evaluate.\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path=args.data_path+args.dataset+'/'\n",
    "    test_set=DialogDataset(data_path+'test.h5', conf['diaglen'], conf['maxlen'])\n",
    "    test_loader=torch.utils.data.DataLoader(dataset=test_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "    vocab = load_dict(data_path+'vocab.json')\n",
    "    n_tokens = len(vocab)\n",
    "\n",
    "    metrics=Metrics()\n",
    "    \n",
    "    # Load model checkpoints    \n",
    "    model = MyModel(conf, n_tokens)\n",
    "    load_model(model, 0)\n",
    "    #model=model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    f_eval = open(\"./output/results.txt\", \"w\")\n",
    "    repeat = args.n_samples\n",
    "    \n",
    "    evaluate(model, metrics, test_loader, vocab, repeat, f_eval)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='PyTorch DialogGAN for Eval')\n",
    "    parser.add_argument('--data_path', type=str, default='./data/', help='location of the data corpus')\n",
    "#     parser.add_argument('--dataset', type=str, default='weibo', help='name of dataset, SWDA or DailyDial')\n",
    "    parser.add_argument('--dataset', type=str, default='dailydialog', help='name of dataset, SWDA or DailyDial')\n",
    "    parser.add_argument('--model', type=str, default='MyModel', help='model name')\n",
    "    parser.add_argument('--reload_from', type=int, default=0, \n",
    "                        help='directory to load models from')\n",
    "    \n",
    "    parser.add_argument('--n_samples', type=int, default=10, help='Number of responses to sampling')\n",
    "    parser.add_argument('--seed', type=int, default=1111, help='random seed')\n",
    "    args = parser.parse_args(args=[])\n",
    "    print(vars(args))\n",
    "    test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "fcdafa320b1bd52415b4da26a3e91d8c55e0b68a992d8fe533e9215398eb0247"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}